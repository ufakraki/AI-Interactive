import os
import sys
import time
import json
from scraper import Scraper
from excel_exporter import ExcelExporter
from utils import setup_logger, validate_url
from site_configs import SiteConfigs

logger = setup_logger("main")

def clear_screen():
    """EkranÄ± temizler"""
    os.system('cls' if os.name == 'nt' else 'clear')

def print_header():
    """Program baÅŸlÄ±k bilgisini gÃ¶sterir"""
    print("="*50)
    print("WEBION - Web Sitesi Veri Ã‡ekme AracÄ±")
    print("Altaion Interactive")
    print("="*50)
    print()

def get_user_input(prompt, validator=None, error_message=None):
    """KullanÄ±cÄ±dan giriÅŸ alÄ±r ve opsiyonel doÄŸrulama yapar"""
    while True:
        value = input(prompt)
        if validator is None or validator(value):
            return value
        print(error_message or "GeÃ§ersiz giriÅŸ. LÃ¼tfen tekrar deneyin.")

def load_config(config_file="config.json"):
    """YapÄ±landÄ±rma dosyasÄ±nÄ± yÃ¼kler"""
    try:
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        logger.error(f"YapÄ±landÄ±rma dosyasÄ± yÃ¼klenirken hata: {str(e)}")
    
    return {
        "recent_urls": [],
        "selector_presets": {}
    }

def save_config(config, config_file="config.json"):
    """YapÄ±landÄ±rma dosyasÄ±nÄ± kaydeder"""
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"YapÄ±landÄ±rma dosyasÄ± kaydedilirken hata: {str(e)}")

def get_site_url(config):
    """KullanÄ±cÄ±dan site URL'sini alÄ±r"""
    recent_urls = config.get("recent_urls", [])
    
    if recent_urls:
        print("Son kullanÄ±lan URL'ler:")
        for i, url in enumerate(recent_urls, 1):
            print(f"{i}. {url}")
        print("0. Yeni URL gir")
        
        choice = get_user_input("SeÃ§iminiz (0-{}): ".format(len(recent_urls)), 
                               lambda x: x.isdigit() and 0 <= int(x) <= len(recent_urls),
                               "GeÃ§ersiz seÃ§im. LÃ¼tfen listeden bir sayÄ± seÃ§in.")
        
        if choice == "0":
            url = get_user_input("Web sitesi URL'si: ", validate_url, "GeÃ§ersiz URL. http:// veya https:// ile baÅŸlamalÄ±dÄ±r.")
        else:
            url = recent_urls[int(choice)-1]
    else:
        url = get_user_input("Web sitesi URL'si: ", validate_url, "GeÃ§ersiz URL. http:// veya https:// ile baÅŸlamalÄ±dÄ±r.")
    
    # URL'yi recent listesine ekle (zaten varsa en Ã¼ste taÅŸÄ±)
    if url in recent_urls:
        recent_urls.remove(url)
    recent_urls.insert(0, url)
    
    # Son 5 URL'yi tut
    config["recent_urls"] = recent_urls[:5]
    save_config(config)
    
    return url

def get_selector_info(url):
    """Sadece firma linklerini tespit eder - Veri seÃ§icileri artÄ±k universal"""
    print("\nğŸ” Web sitesi otomatik analiz ediliyor...")
    print("â³ LÃ¼tfen bekleyin...")
    
    # Otomatik seÃ§ici tespiti
    scraper = Scraper()  # Sadece Selenium kullan
    scraper.set_base_url(url)
    suggestions = scraper.suggest_selectors(url)
    scraper.close()
    
    if not suggestions:
        print("âŒ Otomatik tespit baÅŸarÄ±sÄ±z oldu. Temel seÃ§iciler kullanÄ±lacak.")
        return get_fallback_selectors()
    
    print("âœ… Otomatik tespit tamamlandÄ±!")
    
    # En yÃ¼ksek gÃ¼ven oranÄ±na sahip seÃ§icileri otomatik seÃ§
    company_links_selector = auto_select_best_suggestion(
        "Firma Linkleri", 
        suggestions.get('company_links', [])
    )
    
    if not company_links_selector:
        print("\nâŒ Firma linkleri otomatik tespit edilemedi. Fallback seÃ§iciler kullanÄ±lÄ±yor.")
        return get_fallback_selectors()
    
    # ArtÄ±k veri seÃ§icilerine ihtiyaÃ§ yok - universal toplama kullanÄ±yoruz
    print("\nğŸ¯ UNIVERSAL VERÄ° TOPLAMA MODUna geÃ§ildi!")
    print("   ğŸ“‹ Sistem sayfadaki TÃœM verileri otomatik toplayacak")
    print("   ğŸ“ Telefon, email, web sitesi, adres, sosyal medya vs.")
    print("   ğŸ” CSS seÃ§icileri artÄ±k gerekmiyor!")
    
    # Sayfalama seÃ§icisini otomatik seÃ§
    next_page_selector = auto_select_best_suggestion(
        "Sayfalama", 
        suggestions.get('pagination', [])
    )
    
    # BaÅŸarÄ±lÄ± tespit Ã¶zeti
    print(f"\nğŸ¯ Tespit Ã–zeti:")
    print(f"   ğŸ“‹ Firma linki seÃ§ici: âœ…")
    print(f"   ğŸ“‹ Veri toplama: âœ… UNIVERSAL MOD (TÃ¼m veriler otomatik)")
    print(f"   ğŸ“‹ Sayfalama: {'âœ…' if next_page_selector else 'âŒ (Tek sayfa iÅŸlenecek)'}")
    return company_links_selector, {}, next_page_selector  # BoÅŸ dict dÃ¶ndÃ¼r, artÄ±k gerekmiyor

def get_max_pages():
    """Maksimum sayfa sayÄ±sÄ±nÄ± belirler"""
    max_pages = None  # SÄ±nÄ±rsÄ±z sayfa
    print(f"\nğŸ“„ TÃœM SAYFALAR iÅŸlenecek (sÄ±nÄ±rsÄ±z)")
    print("   (Maksimum veri toplama modu)")
    return max_pages

def run_scraper(url, use_selenium, company_links_selector, selectors, next_page_selector, max_pages):
    """Ana scraping iÅŸlemini Ã§alÄ±ÅŸtÄ±rÄ±r - Her zaman Selenium kullanÄ±r"""
    print(f"\nğŸš€ Veri Ã§ekme iÅŸlemi baÅŸlÄ±yor...")
    print(f"ğŸ“ URL: {url}")
    print(f"ğŸ”§ Mod: Selenium (JavaScript optimized)")
    print(f"ğŸ“„ Maksimum sayfa: {max_pages or 'SÄ±nÄ±rsÄ±z'}")
    
    scraper = None
    try:
        # Scraper'Ä± baÅŸlat (her zaman Selenium)
        scraper = Scraper()
        scraper.set_base_url(url)
        
        # Yeni iÅŸ akÄ±ÅŸÄ± ile tÃ¼m firma verilerini Ã§ek
        all_companies = scraper.scrape_all_companies(
            start_url=url,
            company_links_selector=company_links_selector,
            data_selectors=selectors,
            next_page_selector=next_page_selector,
            max_pages=max_pages
        )
        
        if not all_companies:
            print("âŒ HiÃ§ veri Ã§ekilemedi!")
            return False
        
        print(f"\nâœ… Toplam {len(all_companies)} firma verisi Ã§ekildi!")
        
        # Excel'e aktar
        exporter = ExcelExporter()
        excel_file = exporter.export_to_excel(all_companies)
        
        if excel_file:
            print(f"ğŸ“Š Veriler Excel dosyasÄ±na kaydedildi: {excel_file}")
            
            # Ã–zet bilgileri gÃ¶ster
            show_summary(all_companies)
            return True
        else:
            print("âŒ Excel dosyasÄ± oluÅŸturulamadÄ±!")
            return False
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan durduruldu!")
        logger.info("Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan durduruldu")
        return False
    except Exception as e:
        logger.error(f"Scraping sÄ±rasÄ±nda hata: {str(e)}")
        print(f"âŒ Hata oluÅŸtu: {str(e)}")
        return False
    finally:
        if scraper:
            scraper.close()

def select_from_suggestions(title, suggestions):
    """Ã–nerilen seÃ§iciler arasÄ±ndan kullanÄ±cÄ±ya seÃ§im yaptÄ±rÄ±r"""
    if not suggestions:
        return None
    
    print(f"\n{title} iÃ§in Ã¶neriler:")
    print("0. Manuel giriÅŸ yap")
    
    for i, suggestion in enumerate(suggestions, 1):
        confidence_percent = int(suggestion['confidence'] * 100)
        print(f"{i}. {suggestion['selector']} ({confidence_percent}% gÃ¼ven)")
        print(f"   â””â”€ {suggestion['description']}")
    
    choice = get_user_input(
        f"SeÃ§iminiz (0-{len(suggestions)}): ",
        lambda x: x.isdigit() and 0 <= int(x) <= len(suggestions),
        "GeÃ§ersiz seÃ§im. LÃ¼tfen listeden bir sayÄ± seÃ§in."
    )
    
    choice_num = int(choice)
    if choice_num == 0:
        return None
    
    return suggestions[choice_num - 1]['selector']

def get_manual_selector_info():
    """Manuel seÃ§ici giriÅŸi"""
    print("\nğŸ“ Manuel seÃ§ici giriÅŸi:")
    
    company_links_selector = input("Firma linkleri iÃ§in CSS seÃ§ici: ")
    
    print("\nFirma detay bilgileri iÃ§in seÃ§iciler:")
    selectors = get_manual_data_selectors()
    
    next_page_selector = input("Sonraki sayfa butonu iÃ§in CSS seÃ§ici (opsiyonel): ")
    
    return company_links_selector, selectors, next_page_selector

def get_manual_data_selectors():
    """Manuel veri seÃ§ici giriÅŸi"""
    selectors = {}
    
    fields = [
        "Firma AdÄ±",
        "Telefon", 
        "E-posta",
        "Adres",
        "Web Sitesi",
        "AÃ§Ä±klama"
    ]
    
    for field in fields:
        selector = input(f"{field} iÃ§in CSS seÃ§ici (boÅŸ bÄ±rakÄ±labilir): ").strip()
        if selector:
            selectors[field] = selector
    
    # Ã–zel alanlar
    while True:
        custom_field = input("Ã–zel alan adÄ± (bitirmek iÃ§in boÅŸ bÄ±rakÄ±n): ").strip()
        if not custom_field:
            break
        
        custom_selector = input(f"{custom_field} iÃ§in CSS seÃ§ici: ").strip()
        if custom_selector:
            selectors[custom_field] = custom_selector
    
    return selectors

def show_summary(data):
    """Ã‡ekilen verilerin Ã¶zetini gÃ¶sterir"""
    print(f"\nğŸ“‹ Veri Ã–zeti:")
    print(f"â”œâ”€ Toplam firma sayÄ±sÄ±: {len(data)}")
    
    # Sayfa bazÄ±nda daÄŸÄ±lÄ±m
    pages = set(item.get('Sayfa', 1) for item in data)
    print(f"â”œâ”€ Ä°ÅŸlenen sayfa sayÄ±sÄ±: {len(pages)}")
    
    # Alan doluluk oranlarÄ±
    if data:
        fields = [field for field in data[0].keys() if field not in ['Sayfa', 'SÄ±ra', 'URL', 'Toplam_SÄ±ra']]
        print(f"â”œâ”€ Ã‡ekilen alan sayÄ±sÄ±: {len(fields)}")
        
        for field in fields:
            filled_count = sum(1 for item in data if item.get(field, '').strip())
            percentage = (filled_count / len(data)) * 100
            print(f"â”‚  â”œâ”€ {field}: %{percentage:.1f} dolu")
    
    print(f"â””â”€ âœ… Ä°ÅŸlem tamamlandÄ±!")

def main():
    """Ana program fonksiyonu - TAM OTOMATÄ°K VERÄ° TOPLAMA SÄ°STEMÄ°"""
    clear_screen()
    print_header()
    
    config = load_config()
    
    # Sadece URL al - geri kalan her ÅŸey otomatik
    url = get_site_url(config)
    
    print(f"\nğŸ¤– TAM OTOMATÄ°K VERÄ° TOPLAMA MODUna geÃ§iliyor...")
    print(f"ğŸ“ Hedef Site: {url}")
    print(f"ğŸ¯ Yeni Teknoloji: UNIVERSAL VERÄ° Ã‡IKARMA (CSS seÃ§ici yok!)")
    print(f"ğŸ” Otomatik Toplar: Firma adÄ±, telefon, email, web sitesi, adres, sosyal medya, sektÃ¶r...")
    print(f"ğŸ”§ Motor: Selenium (JavaScript optimized)")
    print(f"ğŸ“Š Ã‡Ä±ktÄ±: Excel dosyasÄ±")
    print("="*60)
    
    # SeÃ§icileri tam otomatik tespit et
    company_links_selector, selectors, next_page_selector = get_selector_info(url)
    
    # Otomatik sayfa limiti
    max_pages = get_max_pages()
    
    print("\nğŸš€ VERÄ° TOPLAMA Ä°ÅLEMÄ° BAÅLIYOR...")
    print("   ğŸ”¥ YENÄ° UNIVERSAL SÄ°STEM: Sayfadaki TÃœM verileri otomatik Ã§Ä±karÄ±r")
    print("   ğŸ“‹ CSS seÃ§icilerine artÄ±k ihtiyaÃ§ yok!")
    print("   ğŸ¯ Maksimum veri kalitesi garantili")
    
    # Scraping iÅŸlemini Ã§alÄ±ÅŸtÄ±r
    success = run_scraper(url, True, company_links_selector, selectors, next_page_selector, max_pages)
    
    if success:
        print("\nğŸ‰ VERÄ° TOPLAMA Ä°ÅLEMÄ° BAÅARIYLA TAMAMLANDI!")
        print("ğŸ“Š Veriler Excel dosyasÄ±na kaydedildi")
        print("ğŸ”¥ Yeni universal sistem kullanÄ±ldÄ± - maksimum veri kalitesi!")
    else:
        print("\nâŒ Ä°ÅŸlem sÄ±rasÄ±nda bir hata oluÅŸtu.")
        print("ğŸ’¡ FarklÄ± bir URL deneyebilir veya tekrar Ã§alÄ±ÅŸtÄ±rabilirsiniz.")
    
    # Devam seÃ§eneÄŸi
    print(f"\n" + "="*60)
    devam = input("BaÅŸka bir web sitesi taramak istiyor musunuz? (E/h): ").lower()
    if devam in ['e', 'evet', 'y', 'yes']:
        main()  # ProgramÄ± yeniden baÅŸlat
    else:
        print("\nğŸ™ Webion kullandÄ±ÄŸÄ±nÄ±z iÃ§in teÅŸekkÃ¼rler!")
        print("ğŸ’¼ Altaion Interactive - Profesyonel Web Scraping Ã‡Ã¶zÃ¼mleri")

def auto_select_best_suggestion(category, suggestions):
    """En yÃ¼ksek gÃ¼ven oranÄ±na sahip seÃ§iciyi otomatik seÃ§er - Agresif mod"""
    if not suggestions:
        return None
    
    # GÃ¼ven oranÄ±na gÃ¶re sÄ±rala
    best_suggestion = max(suggestions, key=lambda x: x.get('confidence', 0))
    
    # Ã‡ok agresif threshold - %50 bile kabul et
    if best_suggestion['confidence'] >= 0.5:  # %50 gÃ¼ven ve Ã¼zeri otomatik kabul (Ã§ok dÃ¼ÅŸÃ¼rÃ¼ldÃ¼)
        print(f"âœ… {category}: {best_suggestion['confidence']:.0%} gÃ¼ven ile otomatik seÃ§ildi")
        print(f"   SeÃ§ici: {best_suggestion['selector']}")
        return best_suggestion['selector']
    else:
        print(f"\nâš ï¸ {category} iÃ§in en iyi tespit: {best_suggestion['confidence']:.0%} gÃ¼ven - Fallback kullanÄ±lacak")
        return None

def get_fallback_selectors():
    """Otomatik tespit baÅŸarÄ±sÄ±z olduÄŸunda kullanÄ±lacak kapsamlÄ± fallback seÃ§iciler"""
    print("ğŸ”„ KapsamlÄ± fallback seÃ§iciler deneniyor...")
    
    # Ã‡ok agresif ve akÄ±llÄ± link seÃ§icileri (debug sonuÃ§larÄ±na gÃ¶re gÃ¼ncellenmiÅŸ)
    fallback_company_links = "a[href*='exhibitor'], a[href*='company'], a[href*='firm'], a[href*='business'], a[href*='profile'], .item a[href], .result a[href], .listing a[href], a[href]:not([href^='mailto:']):not([href^='tel:']):not([href^='#']):not([href^='javascript']):not([href='/'])"
    
    # Ã‡ok kapsamlÄ± sayfalama seÃ§icileri
    fallback_pagination = """
    a:contains('Next'), 
    a:contains('Sonraki'), 
    a:contains('Ä°leri'),
    a:contains('>'),
    a:contains('â†’'),
    .next, 
    .pagination a:last-child, 
    [class*='next'], 
    [class*='forward'],
    [aria-label*='Next'],
    [aria-label*='next'],
    button[aria-label*='Next'],
    button[aria-label*='next'],
    .pager-next,
    .page-next
    """.replace('\n', '').replace(' ', '')
    
    print("ğŸ”— Agresif firma linki seÃ§icileri hazÄ±rlandÄ±")
    print("ğŸ¯ UNIVERSAL VERÄ° TOPLAMA modu - CSS seÃ§icileri artÄ±k gerekmiyor!")
    
    return fallback_company_links, {}, fallback_pagination  # BoÅŸ dict dÃ¶ndÃ¼r

def get_basic_data_selectors():
    """Sadece temel iÅŸ bilgilerini toplar - Sosyal medya ve gereksiz veriler hariÃ§"""
    return {
        # Sadece temel iÅŸ bilgileri
        'Firma AdÄ±': 'h1:not(:contains("Search")):not(:contains("Menu")), h2:not(:contains("Search")), h3:not(:contains("Search")), .company-name, .firm-name, .business-name, .organization, .exhibitor-name, .title:not(:contains("Search")), .name:not(:contains("Search"))',
        'Telefon': 'a[href^="tel:"], .phone, .tel, .telephone, .contact-phone, .mobile, .gsm, [class*="phone"], [class*="tel"]',
        'E-posta': 'a[href^="mailto:"]:not([href*="recommend"]):not([href*="page"]), .email, .mail, .contact-email, .e-mail, [class*="email"], [class*="mail"]',
        'Web Sitesi': 'a[href^="http"]:not([href*="messefrankfurt"]):not([href*="facebook"]):not([href*="twitter"]):not([href*="linkedin"]):not([href*="instagram"]):not([href*="youtube"]):not([href^="mailto:"]):not([href^="tel:"]), .website, .web, .url, .link, [class*="website"], [class*="web"]',
        
        # Adres ve konum bilgileri
        'Adres': '.address, .addr, .location, .contact-address, .full-address, .street-address, [class*="address"], [class*="location"]',
        'Åehir': '.city, .town, .locality, .city-name, [class*="city"], [class*="town"]',
        'Ãœlke': '.country, .nation, .country-name, .nationality, [class*="country"], [class*="nation"]',
        
        # Ä°ÅŸ bilgileri (sadece temel)
        'SektÃ¶r': '.sector, .industry, .business-type, .category, .field, .expertise, [class*="sector"], [class*="industry"], [class*="category"]'
    }

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nProgram kullanÄ±cÄ± tarafÄ±ndan durduruldu.")
    except Exception as e:
        logger.error(f"Program hatasÄ±: {str(e)}")
        print(f"\nBeklenmeyen bir hata oluÅŸtu: {str(e)}")
        print("LÃ¼tfen tekrar deneyin veya destek alÄ±n.")
